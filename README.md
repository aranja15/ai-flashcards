# ğŸ§  AI Flashcards Generator

A full-stack application that lets users generate **AI-powered flashcards** by either:
- Entering a topic
- Uploading a PDF

Built using:
- ğŸ”¥ Frontend: React (TypeScript)
- ğŸ Backend: FastAPI (Python)
- ğŸ¤– AI Model: LLaMA 3 via Ollama (runs locally)

---

## ğŸ“ Project Structure

```
AI-FLASHCARDS/
â”œâ”€â”€ ai-flashcards-backend/     # FastAPI backend with PDF parsing and AI integration
â”‚   â”œâ”€â”€ services/              # Flashcard generation and PDF handling logic
â”‚   â”œâ”€â”€ utils/                 # Ollama chat client (optional)
â”‚   â”œâ”€â”€ main.py                # Entry point
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ai-flashcards-frontend/    # React frontend (TypeScript)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/        # Flashcard UI components
â”‚       â”œâ”€â”€ services/          # API calls to backend
â”‚       â””â”€â”€ App.tsx            # Main frontend logic
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§‘â€ğŸ’» How to Run the Project Locally

### ğŸ”§ Backend Setup (FastAPI + Ollama)

1. **Clone the repo and navigate to backend:**
```bash
cd ai-flashcards-backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

2. **Install and start Ollama locally (Mac/Linux):**
```bash
brew install ollama        # or follow instructions at https://ollama.com
ollama serve               # Terminal 1
ollama run llama3          # Terminal 2
```

3. **Run FastAPI server:**
```bash
uvicorn main:app --reload
```

By default, it runs on `http://localhost:8000`.

---

### ğŸ¨ Frontend Setup (React + TypeScript)

1. **Navigate to frontend:**
```bash
cd ai-flashcards-frontend
npm install
npm start
```

App will be live on `http://localhost:3000`.

---

## âœ… Features

- ğŸ“¤ Upload a PDF or âœï¸ Enter a topic
- ğŸ¤– Generate 5 flashcards from local LLaMA 3 model
- ğŸ“± Click/Tap to reveal answers
- ğŸ¯ Simple and accessible interface

---

## ğŸ›¡ï¸ Environment Notes

- Ollama runs on port `11434`
- CORS enabled for `localhost:3000` in FastAPI
- No external API costs (uses local inference)
- No database used â€“ session-based flashcards

---

## ğŸ§  Inspiration

Personal project to explore:
- LLM integration without cloud APIs
- End-to-end AI UX flow
- PDF NLP basics
